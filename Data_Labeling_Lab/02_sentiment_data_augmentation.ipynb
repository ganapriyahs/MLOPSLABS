{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Snorkel Intro Tutorial: Data Augmentation for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous tutorial, we used Snorkel's LabelModel to create a labeled training set from noisy heuristics. We will now take that labeled data and augment it using Transformation Functions (TFs).\n",
    "\n",
    "Data augmentation is a popular technique for increasing the size of labeled training sets by applying class-preserving transformations. For text, this could mean replacing a word with its synonym. The key is that the transformation shouldn't change the original label (i.e., a positive tweet should remain positive).\n",
    "\n",
    "This tutorial is divided into four parts:\n",
    "\n",
    "1. Loading Labeled Data: We'll start with the labeled training data generated from the previous tutorial.\n",
    "2. Writing Transformation Functions (TFs): We'll write functions to modify tweets while preserving their sentiment.\n",
    "3. Applying TFs to Augment Our Dataset: We'll use a policy to apply these TFs and create a larger, augmented training set.\n",
    "4. Training a Model: We'll train an LSTM model on both the original and augmented datasets to see the impact on performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Loading the Labeled Data\n",
    "\n",
    "This tutorial assumes the data labeling step is complete. We'll start with the labeled, filtered DataFrame (df_train_filtered) and the hard labels (preds_train_filtered) that you generated in the previous step.\n",
    "\n",
    "For completeness, let's re-run the necessary setup code to get us to that starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initial Setup ---\n",
    "%matplotlib inline\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import utils # Your utility functions\n",
    "import nltk\n",
    "import names\n",
    "import tensorflow as tf\n",
    "from snorkel.labeling import labeling_function, PandasLFApplier, LFAnalysis\n",
    "from snorkel.labeling.model import LabelModel\n",
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "from snorkel.utils import probs_to_preds\n",
    "from snorkel.augmentation import transformation_function, RandomPolicy, MeanFieldPolicy, PandasTFApplier\n",
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "DISPLAY_ALL_TEXT = True\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 0 if DISPLAY_ALL_TEXT else 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "DISPLAY_ALL_TEXT = True\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 0 if DISPLAY_ALL_TEXT else 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n",
      "Number of training examples: 1280000\n",
      "Number of test examples: 320000\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "df_train, df_test = utils.load_dataset(csv_path=\"data/sentiment_analysis.csv\")\n",
    "Y_train = df_train[\"label\"].values\n",
    "Y_test = df_test[\"label\"].values\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Number of training examples: {len(df_train)}\")\n",
    "print(f\"Number of test examples: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@chrishasboobs AHHH I HOPE YOUR OK!!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@misstoriblack cool , i have no tweet apps  for my razr 2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@TiannaChaos i know  just family drama. its lame.hey next time u hang out with kim n u guys like have a sleepover or whatever, ill call u</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>School email won't open  and I have geography stuff on there to revise! *Stupid School* :'(</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>upper airways problem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                        text  \\\n",
       "0  @chrishasboobs AHHH I HOPE YOUR OK!!!                                                                                                       \n",
       "1  @misstoriblack cool , i have no tweet apps  for my razr 2                                                                                   \n",
       "2  @TiannaChaos i know  just family drama. its lame.hey next time u hang out with kim n u guys like have a sleepover or whatever, ill call u   \n",
       "3  School email won't open  and I have geography stuff on there to revise! *Stupid School* :'(                                                 \n",
       "4  upper airways problem                                                                                                                       \n",
       "\n",
       "   label  \n",
       "0  0      \n",
       "1  0      \n",
       "2  0      \n",
       "3  0      \n",
       "4  0      "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Writing Transformation Functions (TFs)\n",
    "\n",
    "Transformation Functions (TFs) are functions that take a data point and return a transformed version of it, while preserving the original label. For sentiment analysis, safe transformations include replacing words with synonyms or replacing specific named entities with generic placeholders.\n",
    "\n",
    "Just like LFs, TFs are created with a decorator, transformation_function, and can use preprocessors like spaCy to parse the text first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "\n",
    "spacy = SpacyPreprocessor(text_field=\"text\", doc_field=\"doc\", memoize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import names\n",
    "from snorkel.augmentation import transformation_function\n",
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "\n",
    "replacement_names = [names.get_full_name() for _ in range(50)]\n",
    "\n",
    "@transformation_function(pre=[spacy])\n",
    "def change_person(x):\n",
    "    person_names = [ent.text for ent in x.doc.ents if ent.label_ == \"PERSON\"]\n",
    "    if person_names:\n",
    "        name_to_replace = np.random.choice(person_names)\n",
    "        replacement_name = np.random.choice(replacement_names)\n",
    "        original_text = x.text\n",
    "        x.text = original_text.replace(name_to_replace, replacement_name, 1)\n",
    "        return x if x.text != original_text else None\n",
    "    return None\n",
    "\n",
    "@transformation_function(pre=[spacy])\n",
    "def swap_adjectives(x):\n",
    "    adjective_indices = [i for i, token in enumerate(x.doc) if token.pos_ == \"ADJ\"]\n",
    "    if len(adjective_indices) >= 2:\n",
    "        idx1, idx2 = sorted(np.random.choice(adjective_indices, 2, replace=False))\n",
    "        tokens = list(x.doc)\n",
    "        tokens[idx1], tokens[idx2] = tokens[idx2], tokens[idx1]\n",
    "        new_text_parts = []\n",
    "        for i, token in enumerate(tokens):\n",
    "            new_text_parts.append(token.text)\n",
    "            if i < len(tokens) - 1 and token.whitespace_:\n",
    "                new_text_parts.append(\" \")\n",
    "        x.text = \"\".join(new_text_parts).strip()\n",
    "        return x\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add some transformation functions that use `wordnet` from [NLTK](https://www.nltk.org/) to replace different parts of speech with their synonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from snorkel.augmentation import transformation_function\n",
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet.zip')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# --- Define Helper Functions ---\n",
    "def get_synonym(word, pos=None):\n",
    "    \"\"\"Get a synonym for a word given its part-of-speech (pos).\"\"\"\n",
    "    synsets = wn.synsets(word, pos=pos) #\n",
    "    if synsets:\n",
    "        lemmas = synsets[0].lemmas()\n",
    "        for lemma in lemmas:\n",
    "            synonym = lemma.name().replace(\"_\", \" \") #\n",
    "            if synonym.lower() != word.lower(): #\n",
    "                return synonym\n",
    "    return None\n",
    "\n",
    "def replace_token_with_ws(spacy_doc, idx, replacement):\n",
    "    \"\"\"Replace token at idx, preserving whitespace.\"\"\"\n",
    "    start_text = spacy_doc[:idx].text_with_ws if idx > 0 else \"\"\n",
    "    replacement_with_space = replacement + spacy_doc[idx].whitespace_\n",
    "    end_text = spacy_doc[idx+1:].text if idx+1 < len(spacy_doc) else \"\"\n",
    "    return start_text + replacement_with_space + end_text\n",
    "\n",
    "# --- Define Synonym Replacement TFs ---\n",
    "@transformation_function(pre=[spacy])\n",
    "def replace_verb_with_synonym(x):\n",
    "    \"\"\"Replace a random verb with a synonym.\"\"\"\n",
    "    verb_indices = [i for i, token in enumerate(x.doc) if token.pos_ == \"VERB\"] #\n",
    "    if verb_indices:\n",
    "        idx = np.random.choice(verb_indices) #\n",
    "        synonym = get_synonym(x.doc[idx].text, pos=wn.VERB) #\n",
    "        if synonym: #\n",
    "            x.text = replace_token_with_ws(x.doc, idx, synonym) #\n",
    "            return x #\n",
    "    return None\n",
    "\n",
    "@transformation_function(pre=[spacy])\n",
    "def replace_noun_with_synonym(x):\n",
    "    \"\"\"Replace a random noun with a synonym.\"\"\"\n",
    "    noun_indices = [i for i, token in enumerate(x.doc) if token.pos_ == \"NOUN\"] #\n",
    "    if noun_indices:\n",
    "        idx = np.random.choice(noun_indices) #\n",
    "        synonym = get_synonym(x.doc[idx].text, pos=wn.NOUN) #\n",
    "        if synonym: #\n",
    "            x.text = replace_token_with_ws(x.doc, idx, synonym) #\n",
    "            return x #\n",
    "    return None\n",
    "\n",
    "@transformation_function(pre=[spacy])\n",
    "def replace_adjective_with_synonym(x):\n",
    "    \"\"\"Replace a random adjective with a synonym.\"\"\"\n",
    "    adj_indices = [i for i, token in enumerate(x.doc) if token.pos_ == \"ADJ\"] #\n",
    "    if adj_indices:\n",
    "        idx = np.random.choice(adj_indices) #\n",
    "        synonym = get_synonym(x.doc[idx].text, pos=wn.ADJ) #\n",
    "        if synonym: #\n",
    "            x.text = replace_token_with_ws(x.doc, idx, synonym) #\n",
    "            return x #\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of transformation functions to apply\n",
    "tfs = [\n",
    "    change_person,\n",
    "    swap_adjectives,\n",
    "    replace_verb_with_synonym,\n",
    "    replace_noun_with_synonym,\n",
    "    replace_adjective_with_synonym,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TF Name</th>\n",
       "      <th>Original Text</th>\n",
       "      <th>Transformed Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>change_person</td>\n",
       "      <td>@dennisschaub No but he will be doing pics.  He has the best body   I can't wait!</td>\n",
       "      <td>Manuel Davis No but he will be doing pics.  He has the best body   I can't wait!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>swap_adjectives</td>\n",
       "      <td>my tiny yard ended up being a big project...and I had to evict so many creatures, it broke my heart.  Hoping I didn't kill too many.</td>\n",
       "      <td>my tiny yard ended up being a big project...and I had to evict so manycreatures, it broke my heart.  Hoping I didn't kill too many .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>replace_verb_with_synonym</td>\n",
       "      <td>@oliverclzoff  glad you enjoying yourself!</td>\n",
       "      <td>@oliverclzoff  glad you enjoy yourself!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>replace_noun_with_synonym</td>\n",
       "      <td>got her ear lobe peirced for the third time today and it still hurts</td>\n",
       "      <td>got her ear lobe peirced for the third clip today and it still hurts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>replace_adjective_with_synonym</td>\n",
       "      <td>got her ear lobe peirced for the third time today and it still hurts</td>\n",
       "      <td>got her ear lobe peirced for the 3rd time today and it still hurts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          TF Name  \\\n",
       "0  change_person                    \n",
       "1  swap_adjectives                  \n",
       "2  replace_verb_with_synonym        \n",
       "3  replace_noun_with_synonym        \n",
       "4  replace_adjective_with_synonym   \n",
       "\n",
       "                                                                                                                            Original Text  \\\n",
       "0  @dennisschaub No but he will be doing pics.  He has the best body   I can't wait!                                                        \n",
       "1  my tiny yard ended up being a big project...and I had to evict so many creatures, it broke my heart.  Hoping I didn't kill too many.     \n",
       "2  @oliverclzoff  glad you enjoying yourself!                                                                                               \n",
       "3  got her ear lobe peirced for the third time today and it still hurts                                                                     \n",
       "4  got her ear lobe peirced for the third time today and it still hurts                                                                     \n",
       "\n",
       "                                                                                                                       Transformed Text  \n",
       "0  Manuel Davis No but he will be doing pics.  He has the best body   I can't wait!                                                      \n",
       "1  my tiny yard ended up being a big project...and I had to evict so manycreatures, it broke my heart.  Hoping I didn't kill too many .  \n",
       "2  @oliverclzoff  glad you enjoy yourself!                                                                                               \n",
       "3  got her ear lobe peirced for the third clip today and it still hurts                                                                  \n",
       "4  got her ear lobe peirced for the 3rd time today and it still hurts                                                                    "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import preview_tfs\n",
    "\n",
    "preview_tfs(df_train, tfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table shows examples of different Transformation Functions (TFs) applied to original tweet text to generate augmented data:\n",
    "\n",
    "change_person: Replaced the username @dennisschaub with a randomly generated name, William Watts.\n",
    "\n",
    "swap_adjectives: Swapped the positions of the adjectives \"tiny\" and \"big\".\n",
    "\n",
    "replace_verb_with_synonym: Changed the verb \"enjoying\" to its base form \"enjoy\".\n",
    "\n",
    "replace_noun_with_synonym: Replaced the noun \"project\" with the synonym \"undertaking\".\n",
    "\n",
    "replace_adjective_with_synonym: Replaced the adjective/ordinal \"third\" with its numerical form \"3rd\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 3. Applying Transformation Functions\n",
    "\n",
    " We'll first define a `Policy` to determine what sequence of TFs to apply to each data point.\n",
    "We'll start with a [`RandomPolicy`](https://snorkel.readthedocs.io/en/master/packages/_autosummary/augmentation/snorkel.augmentation.RandomPolicy.html)\n",
    "that samples `sequence_length=2` TFs to apply uniformly at random per data point.\n",
    "The `n_per_original` argument determines how many augmented data points to generate per original data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.augmentation import RandomPolicy\n",
    "\n",
    "random_policy = RandomPolicy(\n",
    "    len(tfs), sequence_length=2, n_per_original=2, keep_original=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, we can do better than uniform random sampling.\n",
    "We might have domain knowledge that some TFs should be applied more frequently than others,\n",
    "or have trained an [automated data augmentation model](https://snorkel.org/blog/tanda/)\n",
    "that learned a sampling distribution for the TFs.\n",
    "Snorkel supports this use case with a\n",
    "[`MeanFieldPolicy`](https://snorkel.readthedocs.io/en/master/packages/_autosummary/augmentation/snorkel.augmentation.MeanFieldPolicy.html),\n",
    "which allows you to specify a sampling distribution for the TFs.\n",
    "We give higher probabilities to the `replace_[X]_with_synonym` TFs, since those provide more information to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.augmentation import MeanFieldPolicy\n",
    "\n",
    "mean_field_policy = MeanFieldPolicy(\n",
    "    len(tfs),\n",
    "    sequence_length=2,\n",
    "    n_per_original=2,\n",
    "    keep_original=True,\n",
    "    p=[0.05, 0.05, 0.3, 0.3, 0.3],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply one or more TFs that we've written to a collection of data points according to our policy, we use a\n",
    "[`PandasTFApplier`](https://snorkel.readthedocs.io/en/master/packages/_autosummary/augmentation/snorkel.augmentation.PandasTFApplier.html)\n",
    "because our data points are represented with a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|â–ˆâ–Ž        | 167792/1280000 [20:14<2:18:46, 133.57it/s]"
     ]
    }
   ],
   "source": [
    "from snorkel.augmentation import PandasTFApplier\n",
    "\n",
    "tf_applier = PandasTFApplier(tfs, mean_field_policy)\n",
    "df_train_augmented = tf_applier.apply(df_train)\n",
    "Y_train_augmented = df_train_augmented[\"label\"].values6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original training set size: {len(df_train)}\")\n",
    "print(f\"Augmented training set size: {len(df_train_augmented)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have almost doubled our dataset using TFs!\n",
    "Note that despite `n_per_original` being set to 2, our dataset may not exactly triple in size,\n",
    "because sometimes TFs return `None` instead of a new data point\n",
    "(e.g. `change_person` when applied to a sentence with no persons).\n",
    "If you prefer to have exact proportions for your dataset, you can have TFs that can't perform a\n",
    "valid transformation return the original data point rather than `None` (as they do here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Training A Model\n",
    "\n",
    "   Our final step is to use the augmented data to train a model. We train an LSTM (Long Short Term Memory) model, which is a very standard architecture for text processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell makes Keras results reproducible. You can ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "session_conf = tf.compat.v1.ConfigProto(\n",
    "    intra_op_parallelism_threads=1, inter_op_parallelism_threads=1\n",
    ")\n",
    "\n",
    "tf.compat.v1.set_random_seed(0)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll train our LSTM on both the original and augmented datasets to compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import featurize_df_tokens, get_keras_lstm\n",
    "\n",
    "X_train = featurize_df_tokens(df_train)\n",
    "X_train_augmented = featurize_df_tokens(df_train_augmented)\n",
    "X_test = featurize_df_tokens(df_test)\n",
    "\n",
    "\n",
    "def train_and_test(X_train, Y_train, X_test=X_test, Y_test=Y_test, num_buckets=30000):\n",
    "    # Define a vanilla LSTM model with Keras\n",
    "    lstm_model = get_keras_lstm(num_buckets)\n",
    "    lstm_model.fit(X_train, Y_train, epochs=5, verbose=0)\n",
    "    preds_test = lstm_model.predict(X_test)[:, 0] > 0.5\n",
    "    return (preds_test == Y_test).mean()\n",
    "\n",
    "\n",
    "acc_augmented = train_and_test(X_train_augmented, Y_train_augmented)\n",
    "acc_original = train_and_test(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test Accuracy (original training data): {100 * acc_original:.1f}%\")\n",
    "print(f\"Test Accuracy (augmented training data): {100 * acc_augmented:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
