{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 Snorkel Intro Tutorial: Data Augmentation for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous tutorial, we used Snorkel's LabelModel to create a labeled training set from noisy heuristics. We will now take that labeled data and augment it using Transformation Functions (TFs).\n",
    "\n",
    "Data augmentation is a popular technique for increasing the size of labeled training sets by applying class-preserving transformations. For text, this could mean replacing a word with its synonym. The key is that the transformation shouldn't change the original label (i.e., a positive tweet should remain positive).\n",
    "\n",
    "This tutorial is divided into four parts:\n",
    "\n",
    "1. Loading Labeled Data: We'll start with the labeled training data generated from the previous tutorial.\n",
    "2. Writing Transformation Functions (TFs): We'll write functions to modify tweets while preserving their sentiment.\n",
    "3. Applying TFs to Augment Our Dataset: We'll use a policy to apply these TFs and create a larger, augmented training set.\n",
    "4. Training a Model: We'll train an LSTM model on both the original and augmented datasets to see the impact on performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Loading the Labeled Data\n",
    "\n",
    "This tutorial assumes the data labeling step is complete. We'll start with the labeled, filtered DataFrame (df_train_filtered) and the hard labels (preds_train_filtered) that you generated in the previous step.\n",
    "\n",
    "For completeness, let's re-run the necessary setup code to get us to that starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initial Setup ---\n",
    "%matplotlib inline\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import utils # Your utility functions\n",
    "import nltk\n",
    "import names\n",
    "import tensorflow as tf\n",
    "from snorkel.labeling import labeling_function, PandasLFApplier, LFAnalysis\n",
    "from snorkel.labeling.model import LabelModel\n",
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "from snorkel.utils import probs_to_preds\n",
    "from snorkel.augmentation import transformation_function, RandomPolicy, MeanFieldPolicy, PandasTFApplier\n",
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed) # Set TF seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", 0) # Display full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and cleaning data...\n",
      "Data loaded and cleaned.\n"
     ]
    }
   ],
   "source": [
    "# --- Reproduce Labeled Data Generation (As before) ---\n",
    "print(\"Loading and cleaning data...\")\n",
    "df_train, df_test = utils.load_dataset(csv_path=\"data/sentiment_analysis.csv\")\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower(); text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'@[^\\s]+', '', text); text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text); return text\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(clean_text)\n",
    "df_test['text'] = df_test['text'].apply(clean_text)\n",
    "df_train['label'] = -1\n",
    "Y_test = df_test[\"label\"].values\n",
    "ABSTAIN = -1; NEGATIVE = 0; POSITIVE = 1;\n",
    "print(\"Data loaded and cleaned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining and applying LFs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1280000/1280000 [00:24<00:00, 51575.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LFs applied.\n"
     ]
    }
   ],
   "source": [
    "print(\"Defining and applying LFs...\")\n",
    "@labeling_function()\n",
    "def positive_keyword_lf(x): return POSITIVE if any(w in x.text for w in [\"love\", \"great\", \"happy\", \"awesome\"]) else ABSTAIN\n",
    "@labeling_function()\n",
    "def negative_keyword_lf(x): return NEGATIVE if any(w in x.text for w in [\"hate\", \"bad\", \"sad\", \"awful\"]) else ABSTAIN\n",
    "@labeling_function()\n",
    "def emoticon_positive_lf(x): return POSITIVE if re.search(r\":\\)|:-\\)|:d|;d\", x.text, re.IGNORECASE) else ABSTAIN\n",
    "@labeling_function()\n",
    "def emoticon_negative_lf(x): return NEGATIVE if re.search(r\":\\(|:-\\(\", x.text, re.IGNORECASE) else ABSTAIN\n",
    "\n",
    "lfs = [positive_keyword_lf, negative_keyword_lf, emoticon_positive_lf, emoticon_negative_lf]\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "print(\"LFs applied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LabelModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 5190.01epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabelModel trained and data filtered.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training LabelModel...\")\n",
    "label_model = LabelModel(cardinality=2, verbose=False)\n",
    "label_model.fit(L_train=L_train, n_epochs=500, seed=seed)\n",
    "probs_train = label_model.predict_proba(L=L_train)\n",
    "df_train_filtered, probs_train_filtered = filter_unlabeled_dataframe(X=df_train, y=probs_train, L=L_train)\n",
    "preds_train_filtered = probs_to_preds(probs=probs_train_filtered)\n",
    "print(\"LabelModel trained and data filtered.\")\n",
    "\n",
    "df_train_labeled = df_train_filtered.copy()\n",
    "df_train_labeled[\"label\"] = preds_train_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created a SUBSET of 10000 labeled examples for augmentation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>872266</th>\n",
       "      <td>ever been in love with someone and cant tell them</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842689</th>\n",
       "      <td>new blog layout is soooo awesome i love it</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391489</th>\n",
       "      <td>tomorrow sounds deadly too bad im in work i cant even go to oasis</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396384</th>\n",
       "      <td>watching trucalling they made a big mistake cancelling this show i loved it</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127284</th>\n",
       "      <td>woah thats cool  just landed in london about 2 and12 hours agoi love the scenery beautiful</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                 text  \\\n",
       "872266   ever been in love with someone and cant tell them                                              \n",
       "842689    new blog layout is soooo awesome i love it                                                    \n",
       "391489    tomorrow sounds deadly too bad im in work i cant even go to oasis                             \n",
       "396384    watching trucalling they made a big mistake cancelling this show i loved it                   \n",
       "1127284   woah thats cool  just landed in london about 2 and12 hours agoi love the scenery beautiful    \n",
       "\n",
       "         label  \n",
       "872266   1      \n",
       "842689   1      \n",
       "391489   0      \n",
       "396384   1      \n",
       "1127284  1      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Create a Small Subset for Augmentation ---\n",
    "subset_size = 10000 # Use 10k examples for faster execution\n",
    "if len(df_train_labeled) > subset_size:\n",
    "    df_train_labeled_subset = df_train_labeled.sample(n=subset_size, random_state=seed)\n",
    "else:\n",
    "    df_train_labeled_subset = df_train_labeled # Use all if less than subset_size\n",
    "\n",
    "Y_train_labeled_subset = df_train_labeled_subset[\"label\"].values\n",
    "\n",
    "print(f\"\\nCreated a SUBSET of {len(df_train_labeled_subset)} labeled examples for augmentation.\")\n",
    "display(df_train_labeled_subset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Writing Transformation Functions (TFs)\n",
    "\n",
    "Transformation Functions (TFs) are functions that take a data point and return a transformed version of it, while preserving the original label. For sentiment analysis, safe transformations include replacing words with synonyms or replacing specific named entities with generic placeholders.\n",
    "\n",
    "Just like LFs, TFs are created with a decorator, transformation_function, and can use preprocessors like spaCy to parse the text first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "\n",
    "spacy = SpacyPreprocessor(text_field=\"text\", doc_field=\"doc\", memoize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up NLTK and SpaCy...\n",
      "SpacyPreprocessor initialized.\n",
      "\n",
      "Defined 6 Transformation Functions.\n",
      "\n",
      "Previewing transformations on subset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TF Name</th>\n",
       "      <th>Original Text</th>\n",
       "      <th>Transformed Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>change_person</td>\n",
       "      <td>finally got to see sixteen candles i love that anthony michael hall dude</td>\n",
       "      <td>finally got to see sixteen candles i love that Kevin North dude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>swap_adjectives</td>\n",
       "      <td>happy half birthday to me  haha oh my gosh im going to be 18 in 183 days</td>\n",
       "      <td>half happy birthday to me  haha oh my gosh im going to be 18 in 183 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>replace_verb_with_synonym</td>\n",
       "      <td>feeling really bad about not going to e3 just found at paul mccartney ringo starr yoko ono were at the ms party for rb the beatles</td>\n",
       "      <td>feeling really bad about not going to e3 just establish at paul mccartney ringo starr yoko ono were at the ms party for rb the beatles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>replace_noun_with_synonym</td>\n",
       "      <td>the bourget air show  awesome performances but now i look like a racoon</td>\n",
       "      <td>the bourget air show  awesome performances but now i look like a raccoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>replace_adjective_with_synonym</td>\n",
       "      <td>the bourget air show  awesome performances but now i look like a racoon</td>\n",
       "      <td>the bourget air show  amazing performances but now i look like a racoon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          TF Name  \\\n",
       "0  change_person                    \n",
       "1  swap_adjectives                  \n",
       "2  replace_verb_with_synonym        \n",
       "3  replace_noun_with_synonym        \n",
       "4  replace_adjective_with_synonym   \n",
       "\n",
       "                                                                                                                          Original Text  \\\n",
       "0  finally got to see sixteen candles i love that anthony michael hall dude                                                               \n",
       "1  happy half birthday to me  haha oh my gosh im going to be 18 in 183 days                                                               \n",
       "2  feeling really bad about not going to e3 just found at paul mccartney ringo starr yoko ono were at the ms party for rb the beatles     \n",
       "3  the bourget air show  awesome performances but now i look like a racoon                                                                \n",
       "4  the bourget air show  awesome performances but now i look like a racoon                                                                \n",
       "\n",
       "                                                                                                                           Transformed Text  \n",
       "0  finally got to see sixteen candles i love that Kevin North dude                                                                           \n",
       "1  half happy birthday to me  haha oh my gosh im going to be 18 in 183 days                                                                  \n",
       "2  feeling really bad about not going to e3 just establish at paul mccartney ringo starr yoko ono were at the ms party for rb the beatles    \n",
       "3  the bourget air show  awesome performances but now i look like a raccoon                                                                  \n",
       "4  the bourget air show  amazing performances but now i look like a racoon                                                                   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# --- Setup: NLTK and spaCy ---\n",
    "print(\"Setting up NLTK and SpaCy...\")\n",
    "try: nltk.data.find('corpora/wordnet.zip')\n",
    "except LookupError: nltk.download('wordnet')\n",
    "\n",
    "try:\n",
    "    spacy = SpacyPreprocessor(text_field=\"text\", doc_field=\"doc\", memoize=True)\n",
    "    print(\"SpacyPreprocessor initialized.\")\n",
    "except IOError: print(\"SpaCy English model not found. Run: !python -m spacy download en_core_web_sm\"); raise\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def get_synonym(word, pos=None):\n",
    "    synsets = wn.synsets(word, pos=pos);\n",
    "    if synsets:\n",
    "        lemmas = synsets[0].lemmas()\n",
    "        for lemma in lemmas:\n",
    "            synonym = lemma.name().replace(\"_\", \" \");\n",
    "            if synonym.lower() != word.lower(): return synonym\n",
    "    return None\n",
    "\n",
    "def replace_token_with_ws(spacy_doc, idx, replacement):\n",
    "    start = spacy_doc[:idx].text_with_ws if idx > 0 else \"\"; rep = replacement + spacy_doc[idx].whitespace_\n",
    "    end = spacy_doc[idx+1:].text if idx+1 < len(spacy_doc) else \"\"; return start + rep + end\n",
    "\n",
    "# --- Define TFs ---\n",
    "replacement_names=[names.get_full_name() for _ in range(50)]\n",
    "@transformation_function(pre=[spacy])\n",
    "def change_person(x):\n",
    "    p=[e.text for e in x.doc.ents if e.label_==\"PERSON\"];\n",
    "    if p: n=np.random.choice(p); r=np.random.choice(replacement_names); o=x.text; x.text=o.replace(n,r,1); return x if x.text!=o else None\n",
    "    return None\n",
    "@transformation_function(pre=[spacy])\n",
    "def swap_adjectives(x):\n",
    "    a=[i for i, t in enumerate(x.doc) if t.pos_==\"ADJ\"];\n",
    "    if len(a)>=2:\n",
    "        i1,i2=sorted(np.random.choice(a,2,replace=False)); t=list(x.doc); t[i1],t[i2]=t[i2],t[i1]\n",
    "        n=\"\".join([tok.text_with_ws for tok in t]).strip(); x.text=n; return x\n",
    "    return None\n",
    "@transformation_function(pre=[spacy])\n",
    "def replace_verb_with_synonym(x):\n",
    "    v=[i for i, t in enumerate(x.doc) if t.pos_==\"VERB\"];\n",
    "    if v: i=np.random.choice(v); s=get_synonym(x.doc[i].text,pos=wn.VERB);\n",
    "    if s: x.text=replace_token_with_ws(x.doc,i,s); return x\n",
    "    return None\n",
    "@transformation_function(pre=[spacy])\n",
    "def replace_noun_with_synonym(x):\n",
    "    n=[i for i, t in enumerate(x.doc) if t.pos_==\"NOUN\"];\n",
    "    if n: i=np.random.choice(n); s=get_synonym(x.doc[i].text,pos=wn.NOUN);\n",
    "    if s: x.text=replace_token_with_ws(x.doc,i,s); return x\n",
    "    return None\n",
    "@transformation_function(pre=[spacy])\n",
    "def replace_adjective_with_synonym(x):\n",
    "    a=[i for i, t in enumerate(x.doc) if t.pos_==\"ADJ\"];\n",
    "    if a: i=np.random.choice(a); s=get_synonym(x.doc[i].text,pos=wn.ADJ);\n",
    "    if s: x.text=replace_token_with_ws(x.doc,i,s); return x\n",
    "    return None\n",
    "@transformation_function()\n",
    "def replace_mention(x): o=x.text; x.text=re.sub(r'(?<!\\w)@[A-Za-z0-9_]+','@user',o,count=1); return x if x.text!=o else None\n",
    "\n",
    "# --- List of TFs ---\n",
    "tfs = [change_person, swap_adjectives, replace_verb_with_synonym, replace_noun_with_synonym, replace_adjective_with_synonym, replace_mention]\n",
    "print(f\"\\nDefined {len(tfs)} Transformation Functions.\")\n",
    "\n",
    "# --- Preview the TFs on the SUBSET ---\n",
    "print(\"\\nPreviewing transformations on subset:\")\n",
    "display(utils.preview_tfs(df_train_labeled_subset.sample(min(100, len(df_train_labeled_subset)), random_state=seed), tfs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add some transformation functions that use `wordnet` from [NLTK](https://www.nltk.org/) to replace different parts of speech with their synonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from snorkel.augmentation import transformation_function\n",
    "# Assuming 'spacy', 'get_synonym', 'replace_token_with_ws' are defined correctly\n",
    "\n",
    "@transformation_function(pre=[spacy])\n",
    "def replace_verb_with_synonym(x):\n",
    "    \"\"\"Replace a random verb with a synonym.\"\"\"\n",
    "    verb_indices = [i for i, token in enumerate(x.doc) if token.pos_ == \"VERB\"]\n",
    "    if verb_indices: # Check if verbs exist first\n",
    "        idx = np.random.choice(verb_indices)\n",
    "        synonym = get_synonym(x.doc[idx].text, pos=wn.VERB)\n",
    "        # --- Nest this check ---\n",
    "        if synonym: # Only proceed if synonym found\n",
    "            x.text = replace_token_with_ws(x.doc, idx, synonym)\n",
    "            return x\n",
    "    # If no verbs OR no synonym found, return None\n",
    "    return None\n",
    "\n",
    "@transformation_function(pre=[spacy])\n",
    "def replace_noun_with_synonym(x):\n",
    "    \"\"\"Replace a random noun with a synonym.\"\"\"\n",
    "    noun_indices = [i for i, token in enumerate(x.doc) if token.pos_ == \"NOUN\"]\n",
    "    if noun_indices: # Check if nouns exist first\n",
    "        idx = np.random.choice(noun_indices)\n",
    "        synonym = get_synonym(x.doc[idx].text, pos=wn.NOUN)\n",
    "        # --- Nest this check ---\n",
    "        if synonym: # Only proceed if synonym found\n",
    "            x.text = replace_token_with_ws(x.doc, idx, synonym)\n",
    "            return x\n",
    "    # If no nouns OR no synonym found, return None\n",
    "    return None\n",
    "\n",
    "@transformation_function(pre=[spacy])\n",
    "def replace_adjective_with_synonym(x):\n",
    "    \"\"\"Replace a random adjective with a synonym.\"\"\"\n",
    "    adj_indices = [i for i, token in enumerate(x.doc) if token.pos_ == \"ADJ\"]\n",
    "    if adj_indices: # Check if adjectives exist first\n",
    "        idx = np.random.choice(adj_indices)\n",
    "        synonym = get_synonym(x.doc[idx].text, pos=wn.ADJ)\n",
    "        # --- Nest this check ---\n",
    "        if synonym: # Only proceed if synonym found\n",
    "            x.text = replace_token_with_ws(x.doc, idx, synonym)\n",
    "            return x\n",
    "    # If no adjectives OR no synonym found, return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined list 'tfs' containing 6 transformation functions.\n"
     ]
    }
   ],
   "source": [
    "# List of transformation functions to apply\n",
    "tfs = [\n",
    "    change_person,\n",
    "    swap_adjectives,                 # Use with caution for sentiment\n",
    "    replace_verb_with_synonym,       # Corrected version\n",
    "    replace_noun_with_synonym,       # Corrected version\n",
    "    replace_adjective_with_synonym,  # Corrected version\n",
    "    replace_mention,\n",
    "]\n",
    "\n",
    "print(f\"Defined list 'tfs' containing {len(tfs)} transformation functions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TF Name</th>\n",
       "      <th>Original Text</th>\n",
       "      <th>Transformed Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>change_person</td>\n",
       "      <td>welcome new quotfollowersquot america is and has been watching iran  keep tweeting we hope the best for you iranelection iran9 cnnfail</td>\n",
       "      <td>welcome best Marcia Prieto is and has been watching iran  keep tweeting we hope the new for you iranelection iran9 cnnfail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>swap_adjectives</td>\n",
       "      <td>welcome new quotfollowersquot america is and has been watching iran  keep tweeting we hope the best for you iranelection iran9 cnnfail</td>\n",
       "      <td>welcome best quotfollowersquot america is and has been watching iran  keep tweeting we hope the new for you iranelection iran9 cnnfail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>replace_verb_with_synonym</td>\n",
       "      <td>glad you enjoying yourself</td>\n",
       "      <td>glad you enjoy yourself</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>replace_noun_with_synonym</td>\n",
       "      <td>my tiny yard ended up being a big projectand i had to evict so many creatures it broke my heart  hoping i didnt kill too many</td>\n",
       "      <td>my tiny pace ended up being a big projectand i had to evict so many creatures it broke my heart  hoping i didnt kill too many</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>replace_adjective_with_synonym</td>\n",
       "      <td>got her ear lobe peirced for the third time today and it still hurts</td>\n",
       "      <td>got her ear lobe peirced for the 3rd time today and it still hurts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          TF Name  \\\n",
       "0  change_person                    \n",
       "1  swap_adjectives                  \n",
       "2  replace_verb_with_synonym        \n",
       "3  replace_noun_with_synonym        \n",
       "4  replace_adjective_with_synonym   \n",
       "\n",
       "                                                                                                                            Original Text  \\\n",
       "0  welcome new quotfollowersquot america is and has been watching iran  keep tweeting we hope the best for you iranelection iran9 cnnfail   \n",
       "1  welcome new quotfollowersquot america is and has been watching iran  keep tweeting we hope the best for you iranelection iran9 cnnfail   \n",
       "2    glad you enjoying yourself                                                                                                             \n",
       "3  my tiny yard ended up being a big projectand i had to evict so many creatures it broke my heart  hoping i didnt kill too many            \n",
       "4  got her ear lobe peirced for the third time today and it still hurts                                                                     \n",
       "\n",
       "                                                                                                                         Transformed Text  \n",
       "0  welcome best Marcia Prieto is and has been watching iran  keep tweeting we hope the new for you iranelection iran9 cnnfail              \n",
       "1  welcome best quotfollowersquot america is and has been watching iran  keep tweeting we hope the new for you iranelection iran9 cnnfail  \n",
       "2    glad you enjoy yourself                                                                                                               \n",
       "3  my tiny pace ended up being a big projectand i had to evict so many creatures it broke my heart  hoping i didnt kill too many           \n",
       "4  got her ear lobe peirced for the 3rd time today and it still hurts                                                                      "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import preview_tfs\n",
    "\n",
    "preview_tfs(df_train, tfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table shows examples of different Transformation Functions (TFs) applied to original tweet text to generate augmented data:\n",
    "\n",
    "change_person: Replaced the username @dennisschaub with a randomly generated name, William Watts.\n",
    "\n",
    "swap_adjectives: Swapped the positions of the adjectives \"tiny\" and \"big\".\n",
    "\n",
    "replace_verb_with_synonym: Changed the verb \"enjoying\" to its base form \"enjoy\".\n",
    "\n",
    "replace_noun_with_synonym: Replaced the noun \"project\" with the synonym \"undertaking\".\n",
    "\n",
    "replace_adjective_with_synonym: Replaced the adjective/ordinal \"third\" with its numerical form \"3rd\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 3. Applying Transformation Functions\n",
    "\n",
    " We'll first define a `Policy` to determine what sequence of TFs to apply to each data point.\n",
    "We'll start with a [`RandomPolicy`](https://snorkel.readthedocs.io/en/master/packages/_autosummary/augmentation/snorkel.augmentation.RandomPolicy.html)\n",
    "that samples `sequence_length=2` TFs to apply uniformly at random per data point.\n",
    "The `n_per_original` argument determines how many augmented data points to generate per original data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.augmentation import RandomPolicy\n",
    "\n",
    "random_policy = RandomPolicy(\n",
    "    len(tfs), sequence_length=2, n_per_original=2, keep_original=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, we can do better than uniform random sampling.\n",
    "We might have domain knowledge that some TFs should be applied more frequently than others,\n",
    "or have trained an [automated data augmentation model](https://snorkel.org/blog/tanda/)\n",
    "that learned a sampling distribution for the TFs.\n",
    "Snorkel supports this use case with a\n",
    "[`MeanFieldPolicy`](https://snorkel.readthedocs.io/en/master/packages/_autosummary/augmentation/snorkel.augmentation.MeanFieldPolicy.html),\n",
    "which allows you to specify a sampling distribution for the TFs.\n",
    "We give higher probabilities to the `replace_[X]_with_synonym` TFs, since those provide more information to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeanFieldPolicy defined with custom probabilities.\n"
     ]
    }
   ],
   "source": [
    "from snorkel.augmentation import MeanFieldPolicy, RandomPolicy\n",
    "\n",
    "# Define the probability distribution for sampling TFs.\n",
    "# The length of 'p' MUST match the length of your 'tfs' list (which is 6).\n",
    "# Example: Lower probability for change_person (index 0) and swap_adjectives (index 1)\n",
    "# Higher probability for synonym replacements (indices 2, 3, 4) and replace_mention (index 5)\n",
    "if len(tfs) == 6:\n",
    "    probabilities = [0.05, 0.05, 0.25, 0.25, 0.25, 0.15]\n",
    "    policy = MeanFieldPolicy(\n",
    "        len(tfs),\n",
    "        sequence_length=2,      # Apply 2 TFs per sequence\n",
    "        n_per_original=2,       # Generate 2 new data points per original\n",
    "        keep_original=True,     # Keep the original data point\n",
    "        p=probabilities,        # Use the specified probabilities\n",
    "    )\n",
    "    print(\"MeanFieldPolicy defined with custom probabilities.\")\n",
    "else:\n",
    "    # Fallback if the tfs list length changed unexpectedly\n",
    "    print(f\"Warning: Number of TFs is {len(tfs)}, but probabilities list assumes 6. Using RandomPolicy as fallback.\")\n",
    "    policy = RandomPolicy(len(tfs), sequence_length=2, n_per_original=2, keep_original=True) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply one or more TFs that we've written to a collection of data points according to our policy, we use a\n",
    "[`PandasTFApplier`](https://snorkel.readthedocs.io/en/master/packages/_autosummary/augmentation/snorkel.augmentation.PandasTFApplier.html)\n",
    "because our data points are represented with a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying Transformation Functions to SUBSET...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:00<00:00, 165.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data augmentation complete on subset.\n",
      "\n",
      "Original subset size: 10000\n",
      "Augmented subset size: 21084\n"
     ]
    }
   ],
   "source": [
    "from snorkel.augmentation import PandasTFApplier\n",
    "\n",
    "# Initialize the TF Applier with your TFs and the chosen policy\n",
    "# Ensure 'tfs' and 'policy' are defined from the previous cells\n",
    "if 'tfs' not in locals():\n",
    "    print(\"Error: 'tfs' list is not defined.\")\n",
    "elif 'policy' not in locals():\n",
    "     print(\"Error: 'policy' is not defined.\")\n",
    "elif 'df_train_labeled_subset' not in locals():\n",
    "    print(\"Error: 'df_train_labeled_subset' is not defined.\")\n",
    "else:\n",
    "    tf_applier = PandasTFApplier(tfs, policy) #\n",
    "\n",
    "    # Apply the TFs to the labeled training data SUBSET\n",
    "    print(\"\\nApplying Transformation Functions to SUBSET...\")\n",
    "    # This step will take some time depending on the subset size and TFs\n",
    "    df_train_augmented_subset = tf_applier.apply(df_train_labeled_subset) #\n",
    "\n",
    "    # Extract the labels from the augmented DataFrame\n",
    "    Y_train_augmented_subset = df_train_augmented_subset[\"label\"].values #\n",
    "    print(\"Data augmentation complete on subset.\")\n",
    "\n",
    "    # Display the change in dataset size\n",
    "    print(f\"\\nOriginal subset size: {len(df_train_labeled_subset)}\")\n",
    "    print(f\"Augmented subset size: {len(df_train_augmented_subset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Featurizing data subsets for LSTM...\n",
      "Featurization complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Featurization for LSTM ---\n",
    "print(\"\\nFeaturizing data subsets for LSTM...\")\n",
    "# Featurize the original subset\n",
    "X_train_labeled_lstm_subset = utils.featurize_df_tokens(df_train_labeled_subset)\n",
    "# Featurize the augmented subset\n",
    "X_train_augmented_lstm_subset = utils.featurize_df_tokens(df_train_augmented_subset)\n",
    "# Featurize the FULL test set for evaluation\n",
    "X_test_lstm = utils.featurize_df_tokens(df_test)\n",
    "print(\"Featurization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Training A Model\n",
    "\n",
    "   Our final step is to use the augmented data to train a model. We train an LSTM (Long Short Term Memory) model, which is a very standard architecture for text processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training on ORIGINAL SUBSET ---\n",
      "Training LSTM model on 10000 examples...\n",
      "Epoch 1/5\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6381 - loss: 0.6895 - val_accuracy: 0.6720 - val_loss: 0.6831\n",
      "Epoch 2/5\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6381 - loss: 0.6841 - val_accuracy: 0.6720 - val_loss: 0.6781\n",
      "Epoch 3/5\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6381 - loss: 0.6805 - val_accuracy: 0.6720 - val_loss: 0.6741\n",
      "Epoch 4/5\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6381 - loss: 0.6776 - val_accuracy: 0.6720 - val_loss: 0.6707\n",
      "Epoch 4: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Evaluating model...\n",
      "\n",
      "Test Accuracy (original subset): 50.0%\n",
      "\n",
      "--- Training on AUGMENTED SUBSET ---\n",
      "Training LSTM model on 21084 examples...\n",
      "Epoch 1/5\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6190 - loss: 0.6886 - val_accuracy: 0.6643 - val_loss: 0.6774\n",
      "Epoch 2/5\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.6426 - loss: 0.6777 - val_accuracy: 0.6643 - val_loss: 0.6689\n",
      "Epoch 3/5\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.6426 - loss: 0.6712 - val_accuracy: 0.6643 - val_loss: 0.6626\n",
      "Epoch 4/5\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.6426 - loss: 0.6664 - val_accuracy: 0.6643 - val_loss: 0.6574\n",
      "Epoch 4: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Evaluating model...\n",
      "\n",
      "Test Accuracy (augmented subset): 50.0%\n",
      "\n",
      "--- Results (Training on Subset, Evaluating on Full Test Set) ---\n",
      "Original Subset Accuracy:  50.0%\n",
      "Augmented Subset Accuracy: 50.0%\n",
      "Improvement from Augmentation (on Subset): 0.0%\n"
     ]
    }
   ],
   "source": [
    "# --- Helper function to Train and Test ---\n",
    "def train_and_test_lstm(X_train, Y_train, X_test, Y_test, num_buckets=30000):\n",
    "    # Get LSTM model from utils.py\n",
    "    model = utils.get_keras_lstm(num_buckets)\n",
    "    print(f\"Training LSTM model on {len(X_train)} examples...\")\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        Y_train,\n",
    "        epochs=5,       # Use a small number of epochs for faster demo\n",
    "        batch_size=64,\n",
    "        verbose=1,      # Show training progress\n",
    "        validation_split=0.1, # Use 10% of training data for validation\n",
    "        callbacks=[utils.get_keras_early_stopping(patience=3)] # Use early stopping\n",
    "    )\n",
    "    print(\"Evaluating model...\")\n",
    "    # Evaluate on the full test set\n",
    "    loss, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    return acc\n",
    "\n",
    "# --- Train and Evaluate Models ---\n",
    "print(\"\\n--- Training on ORIGINAL SUBSET ---\")\n",
    "acc_original_subset = train_and_test_lstm(\n",
    "    X_train_labeled_lstm_subset,\n",
    "    Y_train_labeled_subset, # Labels from the original subset\n",
    "    X_test_lstm,\n",
    "    Y_test # Evaluate on the full test set labels\n",
    ")\n",
    "print(f\"\\nTest Accuracy (original subset): {100 * acc_original_subset:.1f}%\") #\n",
    "\n",
    "print(\"\\n--- Training on AUGMENTED SUBSET ---\")\n",
    "acc_augmented_subset = train_and_test_lstm(\n",
    "    X_train_augmented_lstm_subset,\n",
    "    Y_train_augmented_subset, # Labels from the augmented subset\n",
    "    X_test_lstm,\n",
    "    Y_test # Evaluate on the full test set labels\n",
    ")\n",
    "print(f\"\\nTest Accuracy (augmented subset): {100 * acc_augmented_subset:.1f}%\") #\n",
    "\n",
    "# --- Final Comparison ---\n",
    "print(\"\\n--- Results (Training on Subset, Evaluating on Full Test Set) ---\")\n",
    "print(f\"Original Subset Accuracy:  {100 * acc_original_subset:.1f}%\")\n",
    "print(f\"Augmented Subset Accuracy: {100 * acc_augmented_subset:.1f}%\")\n",
    "improvement = acc_augmented_subset - acc_original_subset\n",
    "print(f\"Improvement from Augmentation (on Subset): {100 * improvement:.1f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
