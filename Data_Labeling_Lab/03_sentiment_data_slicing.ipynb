{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ✂️ Snorkel Intro Tutorial: _Data Slicing_\n",
    "\n",
    "In real-world applications, some model outcomes are often more important than others — e.g. vulnerable cyclist detections in an autonomous driving task, or, in our running **spam** application, potentially malicious link redirects to external websites.\n",
    "\n",
    "Traditional machine learning systems optimize for overall quality, which may be too coarse-grained.\n",
    "Models that achieve high overall performance might produce unacceptable failure rates on critical slices of the data — data subsets that might correspond to vulnerable cyclist detection in an autonomous driving task, or in our running spam detection application, external links to potentially malicious websites.\n",
    "\n",
    "In this tutorial, we:\n",
    "1. **Introduce _Slicing Functions (SFs)_** as a programming interface\n",
    "1. **Monitor** application-critical data subsets\n",
    "2. **Improve model performance** on slices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Labeled Data and Define Slicing Functions (SFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initial Setup ---\n",
    "%matplotlib inline\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import utils # Your utility functions\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x154248d30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reproducibility\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
    "SEED = 123\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure logging and display\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARNING) # Reduce verbose Snorkel logging\n",
    "pd.set_option(\"display.max_colwidth\", 0) # Display full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Labeled Data ---\n",
    "# Note: load_dataset keeps labels for both train and test here\n",
    "df_train, df_test = utils.load_dataset(csv_path=\"data/sentiment_analysis.csv\")\n",
    "\n",
    "# Clean the text (important for SFs too)\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'@[^\\s]+', '', text)\n",
    "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(clean_text)\n",
    "df_test['text'] = df_test['text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1280000 training and 320000 test examples.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ahhh i hope your ok</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cool  i have no tweet apps  for my razr 2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i know  just family drama its lamehey next time u hang out with kim n u guys like have a sleepover or whatever ill call u</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>school email wont open  and i have geography stuff on there to revise stupid school</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>upper airways problem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                         text  \\\n",
       "0   ahhh i hope your ok                                                                                                         \n",
       "1   cool  i have no tweet apps  for my razr 2                                                                                   \n",
       "2   i know  just family drama its lamehey next time u hang out with kim n u guys like have a sleepover or whatever ill call u   \n",
       "3  school email wont open  and i have geography stuff on there to revise stupid school                                          \n",
       "4  upper airways problem                                                                                                        \n",
       "\n",
       "   label  \n",
       "0  0      \n",
       "1  0      \n",
       "2  0      \n",
       "3  0      \n",
       "4  0      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract labels\n",
    "Y_train = df_train[\"label\"].values\n",
    "Y_test = df_test[\"label\"].values\n",
    "\n",
    "# Define labels\n",
    "ABSTAIN = -1; NEGATIVE = 0; POSITIVE = 1;\n",
    "\n",
    "print(f\"Loaded {len(df_train)} training and {len(df_test)} test examples.\")\n",
    "display(df_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing Slicing Functions (SFs)\n",
    "\n",
    "Slicing Functions (SFs) are similar to LFs but output a boolean mask indicating whether a data point belongs to the slice. We'll define a few SFs relevant to sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.slicing import slicing_function\n",
    "\n",
    "# SF for short tweets (similar to short_comment in the original)\n",
    "@slicing_function()\n",
    "def short_tweet(x):\n",
    "    \"\"\"Tweets with fewer than 5 words.\"\"\"\n",
    "    return len(x.text.split()) < 5\n",
    "\n",
    "# SF for tweets containing negation words\n",
    "negation_words = [\"not\", \"no\", \"never\", \"ain't\", \"don't\", \"isn't\", \"can't\", \"won't\"]\n",
    "@slicing_function()\n",
    "def has_negation(x):\n",
    "    \"\"\"Tweets containing common negation words.\"\"\"\n",
    "    return any(word in x.text for word in negation_words)\n",
    "\n",
    "# SF for tweets that are questions\n",
    "@slicing_function()\n",
    "def is_question(x):\n",
    "    \"\"\"Tweets ending with a question mark (after cleaning).\"\"\"\n",
    "    # Check the end after stripping potential trailing spaces\n",
    "    return x.text.strip().endswith(\"?\")\n",
    "\n",
    "# SF using a preprocessor (e.g., TextBlob polarity)\n",
    "from snorkel.preprocess import preprocessor\n",
    "from textblob import TextBlob\n",
    "\n",
    "@preprocessor(memoize=True)\n",
    "def textblob_polarity_score(x):\n",
    "    \"\"\"Adds TextBlob polarity score.\"\"\"\n",
    "    scores = TextBlob(x.text)\n",
    "    x.polarity = scores.sentiment.polarity\n",
    "    return x\n",
    "\n",
    "@slicing_function(pre=[textblob_polarity_score])\n",
    "def high_positive_polarity(x):\n",
    "    \"\"\"Tweets with TextBlob polarity > 0.8\"\"\"\n",
    "    return x.polarity > 0.8\n",
    "\n",
    "# List of all SFs\n",
    "sfs = [\n",
    "    short_tweet,\n",
    "    has_negation,\n",
    "    is_question,\n",
    "    high_positive_polarity\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Slices\n",
    "\n",
    "We can use slice_dataframe to see examples from a specific slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples from the 'has_negation' slice:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 32020.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57814</th>\n",
       "      <td>went to a harley davidson dealer to show some of my art this weekend allot of looks but no sales  ill be judging a tattoo comp next</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77033</th>\n",
       "      <td>itâs start raining now hmm second cloudy day</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171509</th>\n",
       "      <td>just saw quotwallk to rememberquot again and its never get old</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136333</th>\n",
       "      <td>i was  when i found out you were in the same house im a huge fan toolol through my tv show im getting racing known in main stream</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173418</th>\n",
       "      <td>ahh thank god he has leftan angry gay man with a hang over is not my idea of fun on a saturday morninghi 2 all in twitt land</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                       text  \\\n",
       "57814   went to a harley davidson dealer to show some of my art this weekend allot of looks but no sales  ill be judging a tattoo comp next   \n",
       "77033   itâs start raining now hmm second cloudy day                                                                                          \n",
       "171509  just saw quotwallk to rememberquot again and its never get old                                                                        \n",
       "136333  i was  when i found out you were in the same house im a huge fan toolol through my tv show im getting racing known in main stream     \n",
       "173418  ahh thank god he has leftan angry gay man with a hang over is not my idea of fun on a saturday morninghi 2 all in twitt land          \n",
       "\n",
       "        label  \n",
       "57814   0      \n",
       "77033   0      \n",
       "171509  1      \n",
       "136333  0      \n",
       "173418  1      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from snorkel.slicing import slice_dataframe\n",
    "\n",
    "print(\"Examples from the 'has_negation' slice:\")\n",
    "negation_df = slice_dataframe(df_test.sample(500, random_state=SEED), has_negation) # Sample for faster display\n",
    "display(negation_df[['text', 'label']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Monitor Slice Performance with Scorer.score_slices\n",
    "\n",
    "Now, let's train a baseline model and see how it performs overall and on our defined slices. This process works with any model framework.\n",
    "\n",
    "\n",
    "Train a Baseline Classifier\n",
    "\n",
    "- We'll use scikit-learn's LogisticRegression with TF-IDF features as our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurizing data with TF-IDF...\n",
      "Featurization complete.\n",
      "Training baseline Logistic Regression model...\n",
      "Baseline model trained.\n",
      "\n",
      "Baseline Model Overall Test Accuracy: 80.3%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"Featurizing data with TF-IDF...\")\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "# Important: Use the sparse matrix output, do NOT use .todense()\n",
    "X_train, _ = utils.df_to_features(vectorizer, df_train, \"train\")\n",
    "X_test, _ = utils.df_to_features(vectorizer, df_test, \"test\")\n",
    "print(\"Featurization complete.\")\n",
    "\n",
    "print(\"Training baseline Logistic Regression model...\")\n",
    "baseline_model = LogisticRegression(solver=\"liblinear\", C=0.1, random_state=SEED)\n",
    "baseline_model.fit(X=X_train, y=Y_train)\n",
    "print(\"Baseline model trained.\")\n",
    "\n",
    "# Get predictions and probabilities (needed for scorer)\n",
    "preds_test = baseline_model.predict(X_test)\n",
    "probs_test = baseline_model.predict_proba(X_test)\n",
    "\n",
    "# Calculate overall accuracy\n",
    "accuracy_baseline = baseline_model.score(X_test, Y_test)\n",
    "print(f\"\\nBaseline Model Overall Test Accuracy: {accuracy_baseline * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply SFs and Score Slices\n",
    "\n",
    "We apply our SFs to the test set to create the slice matrix S_test, then use Scorer to evaluate the baseline model on each slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying Slicing Functions to test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320000/320000 [01:02<00:00, 5094.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFs applied.\n",
      "\n",
      "Checking slice coverage on the test set (S_test):\n",
      "- Slice 'short_tweet': 39573 examples\n",
      "- Slice 'has_negation': 78197 examples\n",
      "- Slice 'is_question': 0 examples\n",
      "- Slice 'high_positive_polarity': 4099 examples\n",
      "\n",
      "Warning: The following slices are empty in the test set: ['is_question']\n",
      "Scoring will likely fail or produce NaN for these slices. Consider removing them from 'sfs' list if unused.\n",
      "\n",
      "Scoring model performance on slices:\n",
      "\n",
      "Error during scoring: Cannot score empty labels\n",
      "This often happens if one or more slices are empty in the test set.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from snorkel.slicing import PandasSFApplier\n",
    "from snorkel.analysis import Scorer\n",
    "\n",
    "\n",
    "print(\"\\nApplying Slicing Functions to test set...\")\n",
    "applier = PandasSFApplier(sfs)\n",
    "S_test = applier.apply(df_test) # Apply SFs to get slice membership matrix\n",
    "print(\"SFs applied.\")\n",
    "\n",
    "\n",
    "print(\"\\nChecking slice coverage on the test set (S_test):\")\n",
    "empty_slices = []\n",
    "if hasattr(S_test, 'dtype') and S_test.dtype.names: # Check if S_test is a structured array with names\n",
    "    slice_names_in_S_test = S_test.dtype.names\n",
    "    for slice_name in slice_names_in_S_test:\n",
    "        try:\n",
    "            coverage = S_test[slice_name].sum()\n",
    "            print(f\"- Slice '{slice_name}': {coverage} examples\")\n",
    "            if coverage == 0:\n",
    "                empty_slices.append(slice_name)\n",
    "        except KeyError:\n",
    "            print(f\"- Warning: Slice name '{slice_name}' not found in S_test dtype names.\")\n",
    "else:\n",
    "    print(\"Warning: S_test might not be in the expected format (numpy structured array). Cannot reliably check coverage.\")\n",
    "    # Attempt to derive names if it's potentially from an older Snorkel version or different format\n",
    "    if isinstance(S_test, np.ndarray) and len(sfs) == S_test.shape[1]:\n",
    "         slice_names_in_S_test = [sf.name for sf in sfs]\n",
    "         for i, slice_name in enumerate(slice_names_in_S_test):\n",
    "             coverage = S_test[:, i].sum()\n",
    "             print(f\"- Slice '{slice_name}' (assumed index {i}): {coverage} examples\")\n",
    "             if coverage == 0:\n",
    "                 empty_slices.append(slice_name)\n",
    "\n",
    "\n",
    "if empty_slices:\n",
    "    print(f\"\\nWarning: The following slices are empty in the test set: {empty_slices}\")\n",
    "    print(\"Scoring will likely fail or produce NaN for these slices. Consider removing them from 'sfs' list if unused.\")\n",
    "else:\n",
    "    print(\"\\nAll slices appear to have coverage on the test set.\")\n",
    "\n",
    "# Initialize scorer with desired metrics (e.g., accuracy)\n",
    "scorer = Scorer(metrics=[\"accuracy\"])\n",
    "\n",
    "# Score model performance on slices (handles potential errors gracefully)\n",
    "print(\"\\nScoring model performance on slices:\")\n",
    "try:\n",
    "    slice_scores = scorer.score_slices(\n",
    "        S=S_test,\n",
    "        golds=Y_test,       # Ground truth labels\n",
    "        preds=preds_test,   # Model's hard predictions\n",
    "        probs=probs_test,   # Model's predicted probabilities\n",
    "        as_dataframe=True   # Return results as a pandas DataFrame\n",
    "    )\n",
    "    display(slice_scores)\n",
    "except ValueError as e:\n",
    "    print(f\"\\nError during scoring: {e}\")\n",
    "    print(\"This often happens if one or more slices are empty in the test set.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred during scoring: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table shows the baseline model's accuracy on the overall test set and on each specific slice we defined. Look for slices where the accuracy is significantly lower than the overall accuracy – these are areas for potential improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Improve Slice Performance with SliceAwareClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we identified slices where our baseline model performed poorly compared to its overall accuracy. Now, we'll use Slice-based Learning, a technique that adds slice-specific components to our model to improve performance on those challenging subsets. Snorkel implements this via the SliceAwareClassifier.\n",
    "\n",
    "Constructing the SliceAwareClassifier\n",
    "\n",
    "First, we need a base PyTorch model architecture. We'll use the simple Multi-Layer Perceptron (MLP) defined in your utils.py file. Then, we initialize the SliceAwareClassifier.\n",
    "\n",
    "base_architecture: The core PyTorch model (our MLP).\n",
    "\n",
    "head_dim: The output dimension of the base_architecture before its final classification layer. This is used by the slice-specific heads.\n",
    "\n",
    "slice_names: The names of the slices we want the model to be aware of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SliceAwareClassifier initialized successfully using get_pytorch_mlp_base.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from snorkel.slicing import SliceAwareClassifier\n",
    "import utils # Ensure utils.py is saved with the new function\n",
    "\n",
    "# --- Define Model Architecture Parameters ---\n",
    "if 'X_train' not in locals():\n",
    "    print(\"Error: X_train (TF-IDF features) not defined. Please run Step 2 first.\")\n",
    "elif 'sfs' not in locals():\n",
    "    print(\"Error: 'sfs' list not defined. Please define slicing functions first.\")\n",
    "elif 'scorer' not in locals():\n",
    "    print(\"Error: 'scorer' object not defined. Please run Step 2 first.\")\n",
    "else:\n",
    "    bow_dim = X_train.shape[1]  # Input dimension from TF-IDF features\n",
    "    hidden_dim = 128            # Dimension of the MLP's hidden layer and base output\n",
    "\n",
    "    # --- Create the Base MLP Model using the new function ---\n",
    "    # This function should return the MLP *without* the final classification layer\n",
    "    base_mlp = utils.get_pytorch_mlp_base(input_dim=bow_dim, hidden_dim=hidden_dim, num_layers=1) # Using num_layers=1 for simplicity\n",
    "    head_dim_to_use = hidden_dim # This now correctly matches the output of base_mlp\n",
    "\n",
    "    # --- Initialize SliceAwareClassifier ---\n",
    "    slice_model = SliceAwareClassifier(\n",
    "        base_architecture=base_mlp,\n",
    "        head_dim=head_dim_to_use,            # Dimension the slice heads take as input\n",
    "        slice_names=[sf.name for sf in sfs], # List of slice names\n",
    "        scorer=scorer,                       # Scorer object for evaluation\n",
    "    )\n",
    "    print(\"SliceAwareClassifier initialized successfully using get_pytorch_mlp_base.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Slice-Aware DataLoaders\n",
    "\n",
    "We need to apply our Slicing Functions (SFs) to the training data (df_train) to get S_train. Then, we create special PyTorch DataLoader objects that include this slice information alongside the features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Slicing Functions to train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1280000/1280000 [05:17<00:00, 4032.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFs applied to train set.\n",
      "Converting features to PyTorch SPARSE tensors...\n",
      "PyTorch sparse datasets created.\n",
      "Creating slice-aware dataloaders...\n",
      "Dataloaders ready.\n",
      "\n",
      "--- IMPORTANT ---\n",
      "DataLoaders now use SPARSE tensors for features.\n",
      "Ensure your PyTorch model (`base_mlp` in SliceAwareClassifier) is designed to accept sparse input.\n",
      "Standard `nn.Linear` layers may need modification (e.g., using `torch.sparse.mm` or specific layers like `nn.EmbeddingBag`).\n"
     ]
    }
   ],
   "source": [
    "from snorkel.classification.data import DictDataset, DictDataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy.sparse # Needed for sparse matrix check and conversion\n",
    "\n",
    "# --- Helper Function to Convert SciPy Sparse to PyTorch Sparse ---\n",
    "def scipy_sparse_to_torch_sparse(sparse_mx):\n",
    "    \"\"\"Converts a SciPy sparse matrix (COO format) to a PyTorch sparse tensor.\"\"\"\n",
    "    # Ensure matrix is in COOrdinate format\n",
    "    if not isinstance(sparse_mx, scipy.sparse.coo_matrix):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    sparse_mx = sparse_mx.astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64)\n",
    "    )\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    # Use torch.sparse_coo_tensor for modern PyTorch versions\n",
    "    return torch.sparse_coo_tensor(indices, values, shape)\n",
    "\n",
    "# --- Code Block for Preparing Slice-Aware DataLoaders (Sparse Version) ---\n",
    "\n",
    "print(\"Applying Slicing Functions to train set...\")\n",
    "if 'applier' not in locals():\n",
    "    print(\"Error: SF Applier not defined. Please run Step 2 first.\")\n",
    "else:\n",
    "    S_train = applier.apply(df_train) #\n",
    "    print(\"SFs applied to train set.\")\n",
    "\n",
    "    # Convert features to PyTorch *SPARSE* tensors\n",
    "    print(\"Converting features to PyTorch SPARSE tensors...\")\n",
    "    try:\n",
    "        # Check if X_train/X_test are SciPy sparse matrices\n",
    "        if isinstance(X_train, scipy.sparse.spmatrix) and isinstance(X_test, scipy.sparse.spmatrix):\n",
    "            X_train_tensor = scipy_sparse_to_torch_sparse(X_train)\n",
    "            X_test_tensor = scipy_sparse_to_torch_sparse(X_test)\n",
    "        else:\n",
    "            # Handle case where input might not be sparse (e.g., if using dense features)\n",
    "            # This fallback might still cause memory errors if X_train/X_test are large dense numpy arrays\n",
    "            print(\"Warning: X_train or X_test is not a SciPy sparse matrix. Attempting direct FloatTensor conversion.\")\n",
    "            X_train_tensor = torch.FloatTensor(X_train)\n",
    "            X_test_tensor = torch.FloatTensor(X_test)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting features to sparse tensors: {e}.\")\n",
    "        raise # Re-raise error to stop execution\n",
    "\n",
    "    Y_train_tensor = torch.LongTensor(Y_train)\n",
    "    Y_test_tensor = torch.LongTensor(Y_test)\n",
    "\n",
    "    # Note: DictDataset *should* handle sparse tensors in the feature dictionary\n",
    "    train_dataset = DictDataset.from_tensors(X_train_tensor, Y_train_tensor, \"train\")\n",
    "    test_dataset = DictDataset.from_tensors(X_test_tensor, Y_test_tensor, \"test\")\n",
    "    print(\"PyTorch sparse datasets created.\")\n",
    "\n",
    "    # Create slice-aware dataloaders\n",
    "    BATCH_SIZE = 64\n",
    "    print(\"Creating slice-aware dataloaders...\")\n",
    "    if 'slice_model' not in locals():\n",
    "        print(\"Error: slice_model not initialized.\")\n",
    "    else:\n",
    "        # The dataloader should yield batches where X is a sparse tensor\n",
    "        train_dl_slice = slice_model.make_slice_dataloader(\n",
    "            train_dataset, S_train, shuffle=True, batch_size=BATCH_SIZE #\n",
    "        )\n",
    "        if 'S_test' not in locals():\n",
    "            print(\"Error: S_test not defined. Please apply SFs to test set first.\")\n",
    "        else:\n",
    "            test_dl_slice = slice_model.make_slice_dataloader(\n",
    "                test_dataset, S_test, shuffle=False, batch_size=BATCH_SIZE #\n",
    "            )\n",
    "            print(\"Dataloaders ready.\")\n",
    "\n",
    "print(\"\\n--- IMPORTANT ---\")\n",
    "print(\"DataLoaders now use SPARSE tensors for features.\")\n",
    "print(\"Ensure your PyTorch model (`base_mlp` in SliceAwareClassifier) is designed to accept sparse input.\")\n",
    "print(\"Standard `nn.Linear` layers may need modification (e.g., using `torch.sparse.mm` or specific layers like `nn.EmbeddingBag`).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Slice-Aware Model\n",
    "\n",
    "Now we train the SliceAwareClassifier using Snorkel's Trainer. The trainer handles the multi-task learning process automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training SliceAwareClassifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0::   0%|          | 0/20000 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::view' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::view' is only available for these backends: [CPU, MPS, Meta, QuantizedCPU, MkldnnCPU, NestedTensorCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/build/aten/src/ATen/RegisterCPU.cpp:30476 [kernel]\nMPS: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/build/aten/src/ATen/RegisterMPS.cpp:28166 [kernel]\nMeta: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/build/aten/src/ATen/RegisterMeta.cpp:26996 [kernel]\nQuantizedCPU: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/build/aten/src/ATen/RegisterQuantizedCPU.cpp:954 [kernel]\nMkldnnCPU: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/build/aten/src/ATen/RegisterMkldnnCPU.cpp:534 [kernel]\nNestedTensorCPU: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/build/aten/src/ATen/RegisterNestedTensorCPU.cpp:825 [kernel]\nBackendSelect: fallthrough registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/build/aten/src/ATen/RegisterFunctionalization_3.cpp:26243 [kernel]\nNamed: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: fallthrough registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/ConjugateFallback.cpp:21 [kernel]\nNegative: fallthrough registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/native/NegateFallback.cpp:22 [kernel]\nZeroTensor: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/build/aten/src/ATen/RegisterZeroTensor.cpp:164 [kernel]\nADInplaceOrView: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5390 [kernel]\nAutogradOther: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradCPU: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradCUDA: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradHIP: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradXLA: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradMPS: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradIPU: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradXPU: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradHPU: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradVE: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradLazy: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradMTIA: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradPrivateUse1: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradPrivateUse2: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradPrivateUse3: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradMeta: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradNestedTensor: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19508 [kernel]\nTracer: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/TraceType_3.cpp:14885 [kernel]\nAutocastCPU: fallthrough registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/autocast_mode.cpp:321 [backend fallback]\nAutocastXPU: fallthrough registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/autocast_mode.cpp:463 [backend fallback]\nAutocastMPS: fallthrough registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/functorch/BatchRulesViews.cpp:555 [kernel]\nBatchedNestedTensor: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/LegacyBatchingRegistrations.cpp:1079 [kernel]\nVmapMode: fallthrough registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/functorch/TensorWrapper.cpp:207 [backend fallback]\nPythonTLSSnapshot: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m Trainer(n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 8\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mfit(slice_model, [train_dl_slice])\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSliceAwareClassifier training complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/snorkel/classification/training/trainer.py:194\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, dataloaders)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mn_epochs):\n\u001b[1;32m    188\u001b[0m     batches \u001b[38;5;241m=\u001b[39m tqdm(\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_scheduler\u001b[38;5;241m.\u001b[39mget_batches(train_dataloaders)),\n\u001b[1;32m    190\u001b[0m         total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_batches_per_epoch,\n\u001b[1;32m    191\u001b[0m         disable\u001b[38;5;241m=\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mprogress_bar),\n\u001b[1;32m    192\u001b[0m         desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    193\u001b[0m     )\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_num, (batch, dataloader) \u001b[38;5;129;01min\u001b[39;00m batches:\n\u001b[1;32m    195\u001b[0m         X_dict, Y_dict \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m    197\u001b[0m         total_batch_num \u001b[38;5;241m=\u001b[39m epoch_num \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_batches_per_epoch \u001b[38;5;241m+\u001b[39m batch_num\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/snorkel/classification/training/schedulers/shuffled_scheduler.py:44\u001b[0m, in \u001b[0;36mShuffledScheduler.get_batches\u001b[0;34m(self, dataloaders)\u001b[0m\n\u001b[1;32m     41\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(dataloader_indices)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m dataloader_indices:\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(dataloader_iters[index]), dataloaders[index]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/snorkel/classification/data.py:147\u001b[0m, in \u001b[0;36mcollate_dicts\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m field_name, values \u001b[38;5;129;01min\u001b[39;00m X_batch\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Only merge list of tensors\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values[\u001b[38;5;241m0\u001b[39m], Tensor):\n\u001b[0;32m--> 147\u001b[0m         X_batch[field_name] \u001b[38;5;241m=\u001b[39m list_to_tensor(values)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label_name, values \u001b[38;5;129;01min\u001b[39;00m Y_batch\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    150\u001b[0m     Y_batch[label_name] \u001b[38;5;241m=\u001b[39m list_to_tensor(values)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/snorkel/classification/utils.py:24\u001b[0m, in \u001b[0;36mlist_to_tensor\u001b[0;34m(item_list)\u001b[0m\n\u001b[1;32m     21\u001b[0m     item_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(item_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Convert reshape to 1-D tensor and then convert\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m     item_tensor, _ \u001b[38;5;241m=\u001b[39m pad_batch([item\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m item_list])\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m item_tensor\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::view' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::view' is only available for these backends: [CPU, MPS, Meta, QuantizedCPU, MkldnnCPU, NestedTensorCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/build/aten/src/ATen/RegisterCPU.cpp:30476 [kernel]\nMPS: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/build/aten/src/ATen/RegisterMPS.cpp:28166 [kernel]\nMeta: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/build/aten/src/ATen/RegisterMeta.cpp:26996 [kernel]\nQuantizedCPU: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/build/aten/src/ATen/RegisterQuantizedCPU.cpp:954 [kernel]\nMkldnnCPU: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/build/aten/src/ATen/RegisterMkldnnCPU.cpp:534 [kernel]\nNestedTensorCPU: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/build/aten/src/ATen/RegisterNestedTensorCPU.cpp:825 [kernel]\nBackendSelect: fallthrough registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/build/aten/src/ATen/RegisterFunctionalization_3.cpp:26243 [kernel]\nNamed: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: fallthrough registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/ConjugateFallback.cpp:21 [kernel]\nNegative: fallthrough registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/native/NegateFallback.cpp:22 [kernel]\nZeroTensor: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/build/aten/src/ATen/RegisterZeroTensor.cpp:164 [kernel]\nADInplaceOrView: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5390 [kernel]\nAutogradOther: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradCPU: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradCUDA: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradHIP: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradXLA: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradMPS: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradIPU: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradXPU: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradHPU: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradVE: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradLazy: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradMTIA: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradPrivateUse1: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradPrivateUse2: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradPrivateUse3: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradMeta: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19529 [autograd kernel]\nAutogradNestedTensor: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/VariableType_3.cpp:19508 [kernel]\nTracer: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/autograd/generated/TraceType_3.cpp:14885 [kernel]\nAutocastCPU: fallthrough registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/autocast_mode.cpp:321 [backend fallback]\nAutocastXPU: fallthrough registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/autocast_mode.cpp:463 [backend fallback]\nAutocastMPS: fallthrough registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/functorch/BatchRulesViews.cpp:555 [kernel]\nBatchedNestedTensor: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/LegacyBatchingRegistrations.cpp:1079 [kernel]\nVmapMode: fallthrough registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/functorch/TensorWrapper.cpp:207 [backend fallback]\nPythonTLSSnapshot: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.classification import Trainer\n",
    "\n",
    "print(\"\\nTraining SliceAwareClassifier...\")\n",
    "if 'slice_model' not in locals() or 'train_dl_slice' not in locals():\n",
    "    print(\"Error: slice_model or train_dl_slice not initialized. Cannot train.\")\n",
    "else:\n",
    "    trainer = Trainer(n_epochs=3, lr=1e-3, progress_bar=True)\n",
    "    trainer.fit(slice_model, [train_dl_slice])\n",
    "    print(\"SliceAwareClassifier training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the Slice-Aware Model\n",
    "\n",
    "Finally, evaluate the trained SliceAwareClassifier on the test set slices. The score_slices method will report metrics for the main task and for each slice-specific head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating SliceAwareClassifier on test slices:\")\n",
    "if 'slice_model' not in locals() or 'test_dl_slice' not in locals():\n",
    "     print(\"Error: slice_model or test_dl_slice not initialized. Cannot evaluate.\")\n",
    "else:\n",
    "    slice_aware_scores = slice_model.score_slices([test_dl_slice], as_dataframe=True)\n",
    "\n",
    "    if 'slice_scores' in locals():\n",
    "        print(\"\\nComparison with Baseline Model:\")\n",
    "        comparison_df = slice_scores.rename(columns={\"accuracy\": \"baseline_accuracy\"})\n",
    "        score_col_name = 'score'\n",
    "        if score_col_name in slice_aware_scores.columns:\n",
    "             slice_aware_scores['slice_name'] = slice_aware_scores['label'].apply(lambda x: x.replace('task_slice:', '').replace('_pred', '') if 'task_slice:' in x else ('overall' if x == 'task' else x))\n",
    "             # Ensure index alignment before merge\n",
    "             if comparison_df.index.name != 'slice_name':\n",
    "                 comparison_df.index.name = 'slice_name'\n",
    "             comparison_df = comparison_df.reset_index().merge(\n",
    "                 slice_aware_scores[['slice_name', score_col_name]],\n",
    "                 on='slice_name',\n",
    "                 how='left'\n",
    "             )\n",
    "             comparison_df = comparison_df.rename(columns={score_col_name: \"slice_aware_accuracy\"}).set_index('slice_name')\n",
    "             # Select only relevant columns for final display\n",
    "             if 'baseline_accuracy' in comparison_df.columns and 'slice_aware_accuracy' in comparison_df.columns:\n",
    "                 display(comparison_df[['baseline_accuracy', 'slice_aware_accuracy']])\n",
    "             else:\n",
    "                 print(\"Warning: Could not create comparison table due to missing columns.\")\n",
    "                 display(slice_aware_scores)\n",
    "        else:\n",
    "            print(f\"Could not find score column '{score_col_name}' in slice_aware_scores.\")\n",
    "            display(slice_aware_scores)\n",
    "    else:\n",
    "        print(\"\\nBaseline scores not found for comparison. Displaying SliceAwareClassifier scores:\")\n",
    "        display(slice_aware_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
